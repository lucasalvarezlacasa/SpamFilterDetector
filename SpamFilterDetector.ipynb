{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying NLP techniques for a SMS Spam/Ham detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of this project is to create a model can that help us determine if a given message is either spam or ham (not spam). For this, I won't be diving that much on all the theory behind it, I will rather be focused on providing you a basic template and code that you can use for this purpose.\n",
    "<br>We will be using many libraries that are pretty well known by the ML community of Python, such as Pandas, Scikit Learn and NLTK so we don't have to reinvent the wheel in many aspects. Again, I encourage you to dive deeper on each one of them to get a better understanding of the potential supported.\n",
    "<br>For NLTK, after installing it through anaconda, you need to open a jupyter notebook an run the following commands:\n",
    "<br><b> >>>>>> import nltk\n",
    "<br> >>>>>> nltk.download()</b>\n",
    "<br>This will prompt a window in where you can choose to download all the nltk packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Raw Text - Model can't distinguish words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take our data from a dataset provide by Kaggle (you can find it here - https://www.kaggle.com/assumewisely/sms-spam-collection - but I've included it in this project as well). \n",
    "<br>Everything starts by understanding the format of our data and determining HOW we can process that data. Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a look at the raw format of our data\n",
    "file_content = open(\"SMSSpamCollection.tsv\", \"r\").read()\n",
    "# Let's display the first 2000 characters of our file\n",
    "file_content[0:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may realize from this, we have a file composed by multiple text lines (you can tell by the \"\\n\" separator) in where each line is integrated by two columns separated by a tab (\\t). The first column corresponds to the label (either spam or ham) and the second one corresponds to the content of that SMS.\n",
    "<br>In other words, this is a tab separated file.\n",
    "<br> In this case, we can use a simple method from the Pandas library in order to help us out reading the content and managing it in a more organized way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the file with Pandas.\n",
    "import pandas as pd\n",
    "# I wanna see at most 100 characters on each column\n",
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "\n",
    "# A couple of tricks here, our file is not a comma separated file, it's a tab separated file, that's why we need to pass \n",
    "# in the separator. On the other hand, we use header equals to None in order to indicate that there's no header column\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep=\"\\t\", header=None, names=[\"label\", \"body_text\"])\n",
    "\n",
    "# By default head will display us the first 5 rows, we can set as a parameter how many we want to see\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Clean our text and tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading our data, we need to clean it. There are many things that we want to remove from the content, since they won't add any value to our model. We want to focus on words and know the role they play in a message being spam or ham.\n",
    "<br><b>IMPORTANT: if you are aware of these concepts, please jump directly to the (v) section in where I put all of this together</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) Remove punctuation\n",
    "This can be easily achieved thanks to the string libary provided by Python. We can retrieve all of the ASCII characters considered as punctuation in the C locale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    line_with_no_punct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return line_with_no_punct\n",
    "\n",
    "data['body_no_punct'] = data['body_text'].apply(lambda x: remove_punctuation(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii) Tokenization\n",
    "Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.\n",
    "<br>In our case, we will go line by line of our dataframe and split the sms content into words.\n",
    "<br>There are many different ways to tokenize, we can for instance use a regular expression or we can use some help from libraries that already have a solid implementation of it such as NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# I'm transforming each line of text to lowercase so we can have a normalized version of all words and \n",
    "# \"Hello\" or \"hello\" are considered the same word.\n",
    "data['body_tokenized'] = data['body_no_punct'].apply(lambda x: word_tokenize(x.lower()))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii) Remove stopwords\n",
    "A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n",
    "We would not want these words to take up space in our database, or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to stop words. NLTK(Natural Language Toolkit) in python has a list of stopwords stored in 16 different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Let's get our english stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(token_list):\n",
    "    no_stopwords_list = [token for token in token_list if token not in english_stopwords]\n",
    "    return no_stopwords_list\n",
    "\n",
    "data['body_tokenized_no_stopwords'] = data['body_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "# Check what happened on the fifth row to understand a little bit more what stopwords are \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iv) Transforming words to their base form (stemming/lemmatizing)\n",
    "As you might imagine, we might have many different forms of words in our sms messages. We can have words such as \"go\", \"going\", \"gone\", \"goes\" that for us should have all the same meaning (go). For this, we have two processes that can help us transform our words into their base forms:\n",
    "\n",
    "<b>Stemming</b> is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.\n",
    "<br>Stem (root) is the part of the word to which you add inflectional (changing/deriving) affixes such as (-ed,-ize, -s,-de,mis). So stemming a word or sentence may result in words that are not actual words. Stems are created by removing the suffixes or prefixes used with a word.\n",
    "<br>This basically means chopping off the end of the word to leave only the base.\n",
    "<br> For example:\n",
    "<br> Stemming/stemmed -> Stem\n",
    "<br> Electricity/electrical -> Electr\n",
    "<br> Berries/Berry -> Berri\n",
    "<br> Connection/Connected/connective -> Connect\n",
    "\n",
    "<b>Lemmatizing</b> is the process of grouping together the inflected forms of a word so they can be analyzed as a single term, identified by the word's lemma.\n",
    "<br>On other words, using vocabulary analysis of words aiming to remove inflectional endings to return the dictionary form of a word.\n",
    "\n",
    "<br><b>When should you use one or the other?</b> Both of them accomplish the same thing, but as you might imagine, there's a tradeoff between accuracy and speed (stemming will be faster since it chops the end of the words using heuristics but without any understadning of the context in which a word is used and this might lead to words that don't even exist in the dictionary).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For efficiency purposes, I'm going to use Stemming on this example. If you wanted to use lemmatizing, it's super easy\n",
    "# to do it with nltk as well. (link in here: https://www.geeksforgeeks.org/python-lemmatization-with-nltk)\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_words(token_list):\n",
    "    stemmed_words = [ps.stem(word) for word in token_list]\n",
    "    return stemmed_words\n",
    "\n",
    "data[\"cleaned_text\"] = data[\"body_tokenized_no_stopwords\"].apply(lambda x: stem_words(x))\n",
    "data.head()\n",
    "\n",
    "# Check on the first row how \"searching\" was transformed into \"search\" or \"promise\" into \"promis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v) Putting all together in just one function\n",
    "Instead of going step by step, we can create a single function that will help us jump from the original sms content into the cleaned list of tokens that we need.\n",
    "<br>Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I know these imports have been done before, I'm just trying to put all together in one function assuming that the previous\n",
    "# explanations never happened\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove punctuation\n",
    "    text_no_punctuation = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    # Tokenize\n",
    "    token_list = word_tokenize(text_no_punctuation.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    token_list_no_sw = [token for token in token_list if token not in english_stopwords]   \n",
    "    \n",
    "    # Transform words into their root versions through stemming\n",
    "    cleaned_version = [ps.stem(token) for token in token_list_no_sw]\n",
    "    return cleaned_version\n",
    "\n",
    "# Transform our original content into the cleaned version of it\n",
    "data[\"processed_text\"] = data[\"body_text\"].apply(lambda x: clean_text(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows we've used for explaining purposes\n",
    "data = data.drop(['body_no_punct', 'body_tokenized', 'body_tokenized_no_stopwords', 'cleaned_text'], axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Vectorize - convert to numeric form\n",
    "Vectorizing is the process we use in order to encode text as integers to create feature vectors. As you might imagine, models work with integers rather than strings, this is why we need to go through this transformation process.\n",
    "<br>A feature vector is an n-dimensional vector of numerical features that represent some object. In our case, remember that we have a structure in where each row contains the label (classification) and the message. We will convert this structure into a matrix that has one row per message and n-columns, each one of them corresponding to all the possible words (tokens) used in all of our messages. Of course the label will be a column as well.\n",
    "As the values, we will count how many times each one of these words or tokens appear in very single message that we have. As you will see later, what we put as the column value depends on the vectorization algorithm we end up using.\n",
    "<br>\n",
    "<br>For example, let's assume that we only have two messages:\n",
    "<br> - \"hello lucas\" classified as spam\n",
    "<br> - \"nice to meet you lucas\" classified as not spam\n",
    "<br>\n",
    "<br> body_text                | hello | lucas | nice | to | meet | you | label\n",
    "<br> \"hello lucas\"            |   1   |   1   |   0  | 0  | 0    | 0   |  spam\n",
    "<br> \"nice to meet you lucas\" |   0   |   1   |   1  | 1  | 1    | 1   |  ham\n",
    "\n",
    "\n",
    "<br> As you can also imagine, there are many ways of creating this matrix, this is, many vectorization methods. The one I showed before it's called Count Vectorization (it just counts how many time a token appears in that sentence). However, there's also a method called n-grams (https://medium.com/machine-learning-intuition/document-classification-part-2-text-processing-eaa26d16c719) and another one called Term frequency (TF-IDF) which creates the weights by using a mathematical formula rather than just counting them up.\n",
    "<br><b>For this example, I will generate this matrix by using TF-IDF.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# We need to provide a function that will clean our text so that the vectorizer can use it\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "\n",
    "# Let's transform and vectorize the content\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text'])\n",
    "\n",
    "# This method should return as many rows as the total amount of messages we have and an \"x\" amount of columns depending\n",
    "# on all the possible tokens used in our messages\n",
    "print(X_tfidf.shape)\n",
    "\n",
    "# This will give us the matrix that we generated, but as you might see, it's kind of hard to understand\n",
    "print(tfidf_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply TF-IDF to a smaller sample for visualization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Let's take only 20 messages from our pool\n",
    "data_sample = data[0:20]\n",
    "\n",
    "tfidf_vect_sample = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf_sample = tfidf_vect_sample.fit_transform(data_sample['body_text'])\n",
    "print(X_tfidf_sample.shape)\n",
    "print(tfidf_vect_sample.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Sparse Matrix out of this structure\n",
    "Let's now wrap this object into a panda's dataframe so we can visualize it better.\n",
    "<br>Take into consideration that each row index represents to a given message from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now create a dataframe out of this object just to visualize it in a better way\n",
    "X_tfidf_df = pd.DataFrame(X_tfidf_sample.toarray())\n",
    "X_tfidf_df.columns = tfidf_vect_sample.get_feature_names()\n",
    "X_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Feature Engineering\n",
    "This section can be quite big, depending on how well you understand the data that you're working with.\n",
    "<br>We will proceed to fit the model only with the different weights for each word or token present on each message (this is, the TF-IDF matrix). However,I'm pretty sure there are many other metrics that can be relevant as well from our data.\n",
    "<br> For example, we can think of the following relations:\n",
    "<br> - Are longer messages more likely to be spam?\n",
    "<br> - Are messages that contain a lot of punctuation more likely to be spam?\n",
    "<br> - Is there a relation in between messages that contain a lot of capital words with them being spam or not?\n",
    "<br> Like this, our list can be huge, and maybe it's nice that we analyze this ideas and see if it's worth or not to add extra columns with this information. For this tutorial I won't do, I will leave it as it is, but I encourage you to try it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Machine Learning Algorithm - fit/train our model\n",
    "We've come to the most interesting part, we have our data pre-processed and formatted so it can be understood by a machine Learning algorithm.\n",
    "<br>What is a machine learning model? We can think of it as a model that can learn and make associations from data on where we train it and then make predictions based on it. A ML model will be good if it's able not only to adapt to the data we're using to train it with, but also to generalize to data that has never seen before.\n",
    "<br> This type of ML is also called as supervised learning, and it's the approach we will follow in this example.\n",
    "\n",
    "<br>\n",
    "<br>There are a couple of concepts that have to be explained here. First of all is how are we supposed to train our model. For this, we have something called the holdout test set, which is the sample of data not used in fitting (training) a model for the purpose of evaluating the model's ability to generalize unseen data.\n",
    "<br>We'll be primarily using K-fold cross-validation to evaluate our model (https://machinelearningmastery.com/k-fold-cross-validation/). The idea is that the full data set is divided into k-subsets and the holdout method is repeated k times. Each time, one of the k-subsets is used as the test set and the other k-1 subsets are put together to be used to train the model. For each evaluation, we will get a result of the metric we are testing for.\n",
    "\n",
    "<br>\n",
    "<br><b>Which metrics can we consider?</b>\n",
    "<br>In this example, we will cover 3 of them:\n",
    "<br> -Accuracy: basically the number predicted correctly divided by the total number of observations.\n",
    "<br> -Precision: within the context of the problem we're working with, this would be the number predicted as spam that are actually spam divided by the total number predicted as spam.\n",
    "<br> -Recall: this would be the number predicted as spam that are actually spam divided by the total number that are actually as spam.\n",
    "<br>Something really important is that knowing only accuracy is not enough. In the case of recall and precision, we can notice that the numerator in both cases is the same (the amount correctly identified) while the denominator is different.\n",
    "<br><b>If false positives are really costly (this depends on the business), we will want to optimize our model for precision. If false negatives are really costly, we want to optimize the model for recall.</b>\n",
    "\n",
    "\n",
    "<br>\n",
    "<br><b>Ensamble Method</b><br>\n",
    "Random forest (https://en.wikipedia.org/wiki/Random_forest#Algorithm) is one type of a machine learning algorithm that falls into the category of the ensamble learners which use an ensamble method. This is a technique that creates multiple models and then combines them to produce better results than any of the single models individually.\n",
    "<br>You can basically combine multiple weaker models into a stronger one.\n",
    "<br> Random forest is an ensamble learning method that constructs a collection of decision trees and then aggregates the predictions of each tree to determine the final prediction. The number of trees that integrate the model is called \"n_estimators\". Let's say we use 100 of them, basically each one of them is going to vote individually if it thinks the message is spam or ham. We then combine the 100 votes and the majority of them will win (if 60 predicted spam then it will be spam the final prediction).\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# We use n_jobs=-1 so it can parallelize. IMPORTANT: we're using the default parameters here\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# n_splits is the k of the k-folds, for this case we will use 5.\n",
    "k_fold = KFold(n_splits=5)\n",
    "\n",
    "# Take into consideration that we need to send the features (the token matrix) separated from the labels that\n",
    "# correspond to them. In this case, we will measure accuracy\n",
    "cross_val_score(rf, X_tfidf, data['label'], cv=k_fold, scoring=\"accuracy\", n_jobs=-1)\n",
    "\n",
    "# 3 LINES OF CODE IS ALL THAT TOOK US TO IMPLEMENT THE MODEL AND VALIDATE IT WITH K-FOLD CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing multiple parameters\n",
    "As I mentioned before, there are many parameters that can be tuned. For instance, we didn't touch any parameters when defining our Random Forest model. There're basically two parameters that are quite insteresting to mess with: n_estimators and max_depth.\n",
    "<br>There's a popular method in where we can test them, which is known as grid search, and the idea behind it is to iterate through many different values of these parameters and try to determine which is the model that produces the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split our test and train data by taking a 0.2 of it for testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_tfidf, data['label'], test_size=0.2)\n",
    "\n",
    "def train_random_forest(estimators,depth):\n",
    "    # Create the random forest classifier\n",
    "    rf = RandomForestClassifier(n_estimators=estimators, max_depth=depth, n_jobs=-1)\n",
    "    \n",
    "    # Train the model\n",
    "    rf_model = rf.fit(x_train,y_train)\n",
    "    \n",
    "    # Predict generate predctions for the test data\n",
    "    y_pred = rf_model.predict(x_test)\n",
    "    \n",
    "    # Analyze metrics for those predictions\n",
    "    precision,recall,fscore,support = score(y_test,y_pred,pos_label=\"spam\",average=\"binary\")\n",
    "    \n",
    "    # Print them so we can visualize for each iteration\n",
    "    accuracy = round(((y_pred == y_test).sum()/len(y_test)),3)\n",
    "    print(f\"Est: {estimators} / Depth: {depth} ----> \" +  \n",
    "          f\"Precision: {round(precision,3)} / Recall: {round(recall,3)} / Accuracy: {accuracy}\")\n",
    "    \n",
    "    \n",
    "# Create a for loop to test different possibilities\n",
    "for estimators in [10,25,50,75,100]:\n",
    "    for depth in [10,20,30,None]:\n",
    "        train_random_forest(estimators,depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can determine models with high depth (or None, which means that it will be determined for best results) are the ones that work better. n_estimators plays an important role as well, but is not as relevant as depth.\n",
    "<br>In this example, this are the parameters thaat end up generating the model that offers the best results:\n",
    "<br>Est: 75 / Depth: None ----> Precision: 1.0 / Recall: 0.818 / Accuracy: 0.976"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we want to combine k-fold cross validation with grid search?\n",
    "In the previous example, we examined those parameters and the performance results by using a single holdout test method (this means we randomly split the test and train set data but only once)-\n",
    "<br>There's a way in where we can combine k-fold cross validation together with grid search to find the best values for our parameters. Here is so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insteado of us creating the for loops, we can define a dictionary with the parameters we want to test\n",
    "params = {\n",
    "    \"n_estimators\": [10,25,50,75,100],\n",
    "    \"max_depth\": [10,20,30,None]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# We will use a k = 5 for the folds\n",
    "gs = GridSearchCV(rf, params, n_jobs=-1, cv=5)\n",
    "gs_fit = gs.fit(X_tfidf, data['label'])\n",
    "\n",
    "# We want to explore the results. The \"cv_results\" attribute will print out all of the results accross all the\n",
    "# folds accross all the different params. This can get messy, let's try to filter the results and sort them\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values(\"mean_test_score\", ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see one more time, the model that gives the best results is the one high depth level (None) and number of estimators.\n",
    "<br>We are filtering based on the mean_test_score, since this value indicates the average of the accuracy to predict in the test set, this is, the ability that our model has to generalize the trained data to the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "There are many other algorithms we can try with the same purpose, for example Gradient Boosting, but I will leave that for following POCs.\n",
    "<br>Now that we have an idea on how to initialize our model, let's try to predict some made up messages and see what we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our train/test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_tfidf, data['label'], test_size=0.2)\n",
    "\n",
    "# Create the random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=None, n_jobs=-1)\n",
    "\n",
    "# Train the model\n",
    "rf_model = rf.fit(x_train,y_train)\n",
    "\n",
    "# IS THERE A WAY TO PREDICT ON MESSAGES DIFFERENT THAN THE ONES PRESENT IN TRAIN/TEST set?\n",
    "# Let's see what our model thinks from messages it's never seen before\n",
    "unseen_df = pd.DataFrame(columns=[\"body_text\"], data=[\n",
    "    [\"Hello. We detected fraudulent activity on your Apple iCloud account. To reset password: www.123applesupport/1234ds.com\"],\n",
    "    [\"Hello, my name is Lucas\"],\n",
    "    [\"New Craigslists Buyer Message: read using this link: https://craigslist-alaskaUSA/1212.com\"]])\n",
    "\n",
    "# Let's generate the tf-idf matrix for this messages\n",
    "tfidf_unseen_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "\n",
    "# Let's transform and vectorize the content\n",
    "unseen_tfidf = tfidf_unseen_vect.fit_transform(unseen_df['body_text'])\n",
    "\n",
    "\n",
    "# Predict generate predctions for the test data\n",
    "print(unseen_tfidf.shape)\n",
    "y_pred = rf_model.predict(unseen_tfidf)\n",
    "#print(y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
